**What is a random variable**
It is a numerical valued function defined on every element in a sample space.
**Sample space:** The total possible outcome of an experiment
A discrete random variable is a random variable that can take on at most a countable number of possible values.

Properties of discrete random variable:
1. $p(x) \geq 0$
2. $\sum_{x=1}^n p(x) =1$ 
3. $p(a<x<b) =\sum_{x=a}^b p(x)$

Continuous random variable are a random variable that can assume an uncountable set of possible values. A random variable X is defined to be a continuous random variable if a non-ve function f(x) defined for all real $x \epsilon (-\infty <x <\infty)$ have the following properties:

Properties of continuous random variable:
1.$f(x) \geq 0$
2. $\int_{-\infty}^{\infty} f(x) dx = 1$
3. $\int_{a}^{b} f(x) dx = p(a<x<b)$

the function f(x) is called the probability density function of random variable.

==What is an independent Event==

A trial is a single occurrence of an experiment. For example, flipping a coin once or rolling a die once.

A random variable is a variable whose value is subject to variations due to chance. For example, the number of heads in 10 coin flips or the number of sixes in 10 dice rolls.

An outcome is the result of a single trial. For example, getting heads or tails when flipping a coin or getting a 1, 2, 3, 4, 5 or 6 when rolling a die.

A sample space is the set of all possible outcomes of an experiment. For example, the sample space for flipping a coin is {heads, tails} and the sample space for rolling a die is {1, 2, 3, 4, 5, 6}.


An event is any subset of the sample space. For example, getting an even number when rolling a die is an event and its subset is {2, 4, 6}. Another example would be getting at least one head when flipping two coins which can be represented as {HH, HT, TH}.

$$\tan^{-1}(\sqrt{}$$




What is the rth moment about the origin
For Discrete Variable
$E(X^r) = \mu_r^1 = \sum X^rf(x)$
For Continuous Variable
$E(X^r) = \mu_r^1 =\int_{-\infty}^{\infty}X^rf(x)$

What is the rth moment about the **mean** 
For Discrete Variable
$E(X - \mu)^r= \mu_r= \sum (X-\mu)^rf(x)$
For Continuous Variable
$E(X-\mu)^r = \mu_r =\int_{-\infty}^{\infty}(X-\mu)^rf(x)$


PMFs
Sure! Here are the probability mass functions (PMFs) for the Bernoulli, Binomial, Poisson, and Geometric distributions:

1. Bernoulli Distribution:

The Bernoulli distribution represents a single trial with two possible outcomes: success (usually denoted as 1) or failure (usually denoted as 0). The PMF is given by:

$$
P(X=k) = \begin{cases} 
p & \text{if } k=1 \\
1-p & \text{if } k=0 
\end{cases}
$$

Here, \(X\) is a random variable representing the outcome of the trial, and \(p\) is the probability of success.

2. Binomial Distribution:

The Binomial distribution represents the number of successes in a fixed number of independent Bernoulli trials. The PMF is given by:

$$
P(X=k) = \binom{n}{k} \cdot p^k \cdot (1-p)^{n-k}
$$

Here, \(X\) is the random variable representing the number of successes, \(n\) is the number of trials, \(p\) is the probability of success in each trial, and \(\binom{n}{k}\) denotes the binomial coefficient.

3. Poisson Distribution:

The Poisson distribution models the number of events occurring in a fixed interval of time or space, given the average rate of occurrence. The PMF is given by:

$$
P(X=k) = \frac{{e^{-\lambda} \cdot \lambda^k}}{{k!}}
$$

Here, \(X\) is the random variable representing the number of events, \(\lambda\) is the average rate of occurrence, and \(e\) is the base of the natural logarithm.

4. Geometric Distribution:

The Geometric distribution models the number of independent Bernoulli trials needed to obtain the first success. The PMF is given by:

$$
P(X=k) = (1-p)^{k-1} \cdot p
$$

Here, \(X\) is the random variable representing the number of trials needed to obtain the first success, and \(p\) is the probability of success in each trial.
